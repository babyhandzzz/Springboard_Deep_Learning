{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is my thinking in a step-by-step form:\n",
    "## I'm using IMDB review & sentiment dataset (binary classfication problem)\n",
    " 1. Clean the text data:\n",
    "    * lowercase everything\n",
    "    * remove any weird symbols like \"<\",\"/\", \"\\\" etc.\n",
    "\n",
    "2.  Split the data into training and testing sets.\n",
    "    * I have done by using sklearn (separate file, not included in this one)\n",
    "\n",
    "3. Load the data using torchtext native methods:\n",
    "    * Create fields\n",
    "    * Load data using fields and create TabularDataset\n",
    "\n",
    "4. Build vocabulary using training dataset, this is where we finally use GloVe.\n",
    "    * we also make sure that these vectors don't get updated (requires_grad = False)\n",
    "\n",
    "5. Using iterators for batching the data, for this one we have to again use torchtext native BucketIterator\n",
    "\n",
    "6. Model, loss function and optimizer are then defined, it looks like the model is working but I don't understand what's going inside.\n",
    "____\n",
    "\n",
    "This is what I have issues with:\n",
    "\n",
    "1. When vocab. is created as far as I understand, we assign a unique code to every unique word which then is being used to map the code to the corresponding vector. Is this what happens?\n",
    "\n",
    "2. Within every bacth there is $n$ number of reviews, each review has $t$ number of words. Essentially the only dimension that has no variation is the length of the embedding vector (50 in my case). What I fail to understand is what is the shape of our input for the first layer. As I don't undesrstand the ipnut I am lost within the rest of the NN.\n",
    "\n",
    "3. Why do we need to keep track of $c_t$ and $h_t$ ?\n",
    "https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM\n",
    "\n",
    "### Anyway, If we could go over the flow of tensors within the network that would really help me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field, TabularDataset, Iterator, BucketIterator\n",
    "from torchtext.vocab import GloVe as glove\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim \n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/Users/babyhandzzz/Desktop/ELEPH@NT/Datasets/IMDB_sentiment_analysis'\n",
    "\n",
    "# this things are required by the torchtext module for effiicent data loading.\n",
    "TEXT = Field(sequential=True)\n",
    "LABEL = Field(sequential=False)\n",
    "fields = [('review', TEXT), ('sentiment', LABEL)] \n",
    "\n",
    "\n",
    "train, test = TabularDataset.splits(path=folder,\n",
    "train = 'train.csv',\n",
    "test = 'test.csv',\n",
    "format = 'csv',\n",
    "skip_header=True,\n",
    "fields=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part builds vocabulary using GLoVe's vectors of length 50.  \n",
    "TEXT.build_vocab(train,vectors=glove(name='6B', dim=50), max_size=24000, min_freq=10)\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one is a torchtext native iterator that essentially replace dataloader class within standard Pytorch librabry.\n",
    "train_iter, test_iter = BucketIterator.splits((train, test), batch_size=1) # don't mind batch size of 1, I'm just trying to make it work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(len(TEXT.vocab.stoi), 50, 100)\n",
    "model.embeddings.weight.data=TEXT.vocab.vectors # the missing link\n",
    "model.embeddings.weight.requires_grad = False\n",
    "optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad==True], lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[-0.0173]],\n\n        [[-0.0295]],\n\n        [[-0.0476]],\n\n        [[-0.0445]],\n\n        [[-0.0166]],\n\n        [[-0.0489]],\n\n        [[ 0.0053]],\n\n        [[-0.0242]],\n\n        [[ 0.0244]],\n\n        [[-0.0551]],\n\n        [[-0.0052]],\n\n        [[-0.0532]],\n\n        [[-0.0267]],\n\n        [[-0.0585]],\n\n        [[-0.0022]],\n\n        [[ 0.0205]],\n\n        [[-0.0674]],\n\n        [[-0.0325]],\n\n        [[-0.0362]],\n\n        [[-0.0511]],\n\n        [[-0.0139]],\n\n        [[ 0.0226]],\n\n        [[-0.0203]],\n\n        [[ 0.0133]],\n\n        [[ 0.0444]],\n\n        [[-0.0277]],\n\n        [[ 0.0023]],\n\n        [[-0.0205]],\n\n        [[-0.0623]],\n\n        [[ 0.0248]],\n\n        [[ 0.0094]],\n\n        [[-0.0014]],\n\n        [[-0.0248]],\n\n        [[-0.0419]],\n\n        [[ 0.0376]],\n\n        [[ 0.0357]],\n\n        [[ 0.0054]],\n\n        [[ 0.0212]],\n\n        [[-0.0440]],\n\n        [[-0.0026]],\n\n        [[-0.0316]],\n\n        [[-0.0597]],\n\n        [[ 0.0047]],\n\n        [[-0.0606]],\n\n        [[-0.0285]],\n\n        [[ 0.0046]],\n\n        [[-0.0565]],\n\n        [[-0.0378]],\n\n        [[ 0.0149]],\n\n        [[-0.0232]],\n\n        [[-0.0686]],\n\n        [[-0.0081]],\n\n        [[-0.0816]],\n\n        [[ 0.0148]],\n\n        [[-0.0122]],\n\n        [[-0.0772]],\n\n        [[ 0.0187]],\n\n        [[-0.0217]],\n\n        [[-0.0169]],\n\n        [[ 0.0146]],\n\n        [[ 0.0014]],\n\n        [[ 0.0035]],\n\n        [[ 0.0027]],\n\n        [[-0.0157]],\n\n        [[-0.0358]],\n\n        [[-0.0400]],\n\n        [[ 0.0101]],\n\n        [[-0.0281]],\n\n        [[-0.0656]],\n\n        [[-0.0388]],\n\n        [[ 0.0228]],\n\n        [[-0.0235]],\n\n        [[-0.0390]],\n\n        [[-0.0430]],\n\n        [[-0.0449]],\n\n        [[ 0.0190]],\n\n        [[-0.0036]],\n\n        [[ 0.0028]],\n\n        [[-0.0281]],\n\n        [[-0.0206]],\n\n        [[-0.0752]],\n\n        [[ 0.0175]],\n\n        [[ 0.0053]],\n\n        [[-0.0082]],\n\n        [[ 0.0050]],\n\n        [[-0.0392]],\n\n        [[-0.0020]],\n\n        [[ 0.0044]],\n\n        [[-0.0014]],\n\n        [[-0.0236]],\n\n        [[-0.0544]],\n\n        [[-0.0054]],\n\n        [[-0.0033]],\n\n        [[-0.0608]],\n\n        [[ 0.0200]],\n\n        [[-0.0562]],\n\n        [[-0.0256]],\n\n        [[-0.0869]],\n\n        [[-0.0006]],\n\n        [[ 0.0072]],\n\n        [[ 0.0615]],\n\n        [[-0.0565]],\n\n        [[ 0.0100]],\n\n        [[-0.0109]],\n\n        [[ 0.0039]],\n\n        [[-0.0519]],\n\n        [[ 0.0364]],\n\n        [[-0.0002]],\n\n        [[ 0.0230]],\n\n        [[-0.0213]],\n\n        [[ 0.0010]],\n\n        [[-0.0299]],\n\n        [[-0.0033]],\n\n        [[-0.0291]],\n\n        [[-0.0645]],\n\n        [[-0.0139]],\n\n        [[ 0.0050]],\n\n        [[-0.0467]],\n\n        [[-0.0432]],\n\n        [[-0.0481]],\n\n        [[ 0.0005]],\n\n        [[-0.0330]],\n\n        [[-0.0584]],\n\n        [[ 0.0298]],\n\n        [[-0.0721]],\n\n        [[ 0.0262]],\n\n        [[-0.0410]],\n\n        [[ 0.0283]],\n\n        [[ 0.0006]],\n\n        [[ 0.0150]],\n\n        [[ 0.0409]],\n\n        [[ 0.0013]],\n\n        [[ 0.0349]],\n\n        [[-0.0542]],\n\n        [[ 0.0108]],\n\n        [[ 0.0028]],\n\n        [[ 0.0038]],\n\n        [[ 0.0040]],\n\n        [[-0.0409]],\n\n        [[-0.0533]],\n\n        [[ 0.0276]],\n\n        [[-0.0751]],\n\n        [[ 0.0363]],\n\n        [[ 0.0069]],\n\n        [[ 0.0351]],\n\n        [[-0.0346]],\n\n        [[-0.0128]],\n\n        [[-0.0080]],\n\n        [[-0.0134]],\n\n        [[ 0.0058]],\n\n        [[-0.0345]],\n\n        [[-0.0437]],\n\n        [[-0.0437]],\n\n        [[ 0.0083]],\n\n        [[-0.0225]],\n\n        [[ 0.0040]],\n\n        [[-0.0234]],\n\n        [[-0.0163]],\n\n        [[-0.0452]],\n\n        [[-0.0761]],\n\n        [[-0.0259]],\n\n        [[-0.0317]],\n\n        [[-0.0117]],\n\n        [[ 0.0066]],\n\n        [[ 0.0098]],\n\n        [[-0.0314]],\n\n        [[-0.0723]],\n\n        [[ 0.0011]],\n\n        [[-0.0342]],\n\n        [[ 0.0049]],\n\n        [[-0.0270]],\n\n        [[-0.0085]],\n\n        [[-0.0004]],\n\n        [[-0.0090]],\n\n        [[ 0.0076]],\n\n        [[-0.0330]],\n\n        [[-0.0698]],\n\n        [[-0.0047]],\n\n        [[-0.0121]],\n\n        [[ 0.0092]],\n\n        [[-0.0240]],\n\n        [[ 0.0082]],\n\n        [[ 0.0115]],\n\n        [[ 0.0056]],\n\n        [[-0.0156]],\n\n        [[-0.0058]],\n\n        [[-0.0113]],\n\n        [[-0.0374]],\n\n        [[-0.0026]],\n\n        [[-0.0543]]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "model.train()\n",
    "for i in range(1):\n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        model.zero_grad()\n",
    "        indices, target = batch.review, batch.sentiment\n",
    "        out = model(indices)\n",
    "        print(out)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}